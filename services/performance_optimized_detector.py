"""
æ€§èƒ½ä¼˜åŒ–çš„æ¨¡å—æ£€æµ‹å™¨
é›†æˆé«˜çº§ç¼“å­˜ç®¡ç†å’Œå¹¶å‘å¤„ç†
"""

import os
import sys
import time
import hashlib
import threading
from typing import Set, Dict, List, Optional, Callable, Any, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from pathlib import Path

# å¯¼å…¥ç¼“å­˜ç®¡ç†å™¨å’Œå…¶ä»–æ£€æµ‹å™¨
from utils.advanced_cache_manager import AdvancedCacheManager
from .intelligent_module_analyzer import IntelligentModuleAnalyzer, IntelligentAnalysisResult
from .advanced_dynamic_detector import AdvancedDynamicDetector


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    total_time: float = 0.0
    cache_time: float = 0.0
    detection_time: float = 0.0
    merge_time: float = 0.0
    cache_hit_rate: float = 0.0
    memory_usage: int = 0
    cpu_usage: float = 0.0
    parallel_efficiency: float = 0.0
    
    def __str__(self) -> str:
        return f"""æ€§èƒ½æŒ‡æ ‡:
  æ€»è€—æ—¶: {self.total_time:.3f}s
  ç¼“å­˜è€—æ—¶: {self.cache_time:.3f}s
  æ£€æµ‹è€—æ—¶: {self.detection_time:.3f}s
  åˆå¹¶è€—æ—¶: {self.merge_time:.3f}s
  ç¼“å­˜å‘½ä¸­ç‡: {self.cache_hit_rate:.1%}
  å†…å­˜ä½¿ç”¨: {self.memory_usage / 1024 / 1024:.1f}MB
  CPUä½¿ç”¨ç‡: {self.cpu_usage:.1%}
  å¹¶è¡Œæ•ˆç‡: {self.parallel_efficiency:.1%}"""


@dataclass
class OptimizedDetectionResult:
    """ä¼˜åŒ–æ£€æµ‹ç»“æœ"""
    # æ£€æµ‹ç»“æœ
    analysis_result: IntelligentAnalysisResult
    
    # æ€§èƒ½æŒ‡æ ‡
    performance_metrics: PerformanceMetrics
    
    # ç¼“å­˜ä¿¡æ¯
    cache_key: str
    cache_hit: bool
    cache_source: str  # memory, disk, database, none
    
    # å¹¶å‘ä¿¡æ¯
    parallel_tasks: int
    failed_tasks: int
    
    # ä¼˜åŒ–å»ºè®®
    optimization_suggestions: List[str] = field(default_factory=list)


class PerformanceOptimizedDetector:
    """æ€§èƒ½ä¼˜åŒ–çš„æ¨¡å—æ£€æµ‹å™¨"""
    
    def __init__(self, 
                 python_interpreter: str = "",
                 cache_dir: str = "cache",
                 max_workers: int = None,
                 enable_process_pool: bool = False,
                 cache_ttl: int = 3600):
        
        self.python_interpreter = python_interpreter or sys.executable
        self.max_workers = max_workers or min(8, (os.cpu_count() or 1) + 4)
        self.enable_process_pool = enable_process_pool
        
        # åˆå§‹åŒ–é«˜çº§ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = AdvancedCacheManager(
            cache_dir=cache_dir,
            max_memory_size=200 * 1024 * 1024,  # 200MB
            max_memory_entries=2000,
            default_ttl=cache_ttl
        )
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.intelligent_analyzer = IntelligentModuleAnalyzer(
            python_interpreter=python_interpreter,
            timeout=30
        )
        
        # æ€§èƒ½ç›‘æ§
        self.performance_lock = threading.Lock()
        self.total_requests = 0
        self.total_time = 0.0
        self.cache_hits = 0
        
        # é¢„çƒ­ç¼“å­˜
        self._warmup_cache()
    
    def detect_modules_optimized(self, 
                                script_path: str,
                                output_callback: Optional[Callable[[str], None]] = None,
                                use_execution: bool = True,
                                force_refresh: bool = False,
                                enable_profiling: bool = False) -> OptimizedDetectionResult:
        """æ‰§è¡Œä¼˜åŒ–çš„æ¨¡å—æ£€æµ‹"""
        
        start_time = time.time()
        metrics = PerformanceMetrics()
        
        if output_callback:
            output_callback("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–çš„æ¨¡å—æ£€æµ‹...")
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(script_path, use_execution)
        
        # å°è¯•ä»ç¼“å­˜è·å–ç»“æœ
        cache_start = time.time()
        cached_result = None
        cache_source = "none"
        
        if not force_refresh:
            cached_result = self.cache_manager.get(cache_key)
            if cached_result:
                cache_source = "memory"  # ç®€åŒ–ï¼Œå®é™…å¯ä»¥æ›´è¯¦ç»†
                if output_callback:
                    output_callback("ğŸ’¾ ä»ç¼“å­˜åŠ è½½æ£€æµ‹ç»“æœ")
        
        metrics.cache_time = time.time() - cache_start
        
        if cached_result:
            # ç¼“å­˜å‘½ä¸­
            metrics.cache_hit_rate = 1.0
            with self.performance_lock:
                self.cache_hits += 1
            
            result = OptimizedDetectionResult(
                analysis_result=cached_result,
                performance_metrics=metrics,
                cache_key=cache_key,
                cache_hit=True,
                cache_source=cache_source,
                parallel_tasks=0,
                failed_tasks=0
            )
            
            metrics.total_time = time.time() - start_time
            return result
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæ£€æµ‹
        detection_start = time.time()
        
        if enable_profiling:
            try:
                import psutil
                process = psutil.Process()
                initial_memory = process.memory_info().rss
                initial_cpu = process.cpu_percent()
            except ImportError:
                # psutil ä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½ç›‘æ§
                enable_profiling = False
                initial_memory = 0
                initial_cpu = 0.0
        
        # å¹¶è¡Œæ‰§è¡Œæ£€æµ‹
        analysis_result, parallel_info = self._execute_parallel_detection(
            script_path, output_callback, use_execution
        )
        
        metrics.detection_time = time.time() - detection_start
        
        # åˆå¹¶ç»“æœ
        merge_start = time.time()
        # è¿™é‡Œå¯ä»¥æ·»åŠ é¢å¤–çš„åˆå¹¶é€»è¾‘
        metrics.merge_time = time.time() - merge_start
        
        # å¼‚æ­¥ç¼“å­˜ç»“æœ
        self._async_cache_result(cache_key, analysis_result)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if enable_profiling:
            try:
                final_memory = process.memory_info().rss
                final_cpu = process.cpu_percent()
                metrics.memory_usage = final_memory - initial_memory
                metrics.cpu_usage = (initial_cpu + final_cpu) / 2
            except:
                # æ€§èƒ½ç›‘æ§å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                metrics.memory_usage = 0
                metrics.cpu_usage = 0.0
        
        metrics.total_time = time.time() - start_time
        metrics.cache_hit_rate = 0.0
        metrics.parallel_efficiency = self._calculate_parallel_efficiency(
            metrics.detection_time, parallel_info['parallel_tasks']
        )
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_suggestions = self._generate_optimization_suggestions(
            metrics, analysis_result, parallel_info
        )
        
        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        with self.performance_lock:
            self.total_requests += 1
            self.total_time += metrics.total_time
        
        result = OptimizedDetectionResult(
            analysis_result=analysis_result,
            performance_metrics=metrics,
            cache_key=cache_key,
            cache_hit=False,
            cache_source="none",
            parallel_tasks=parallel_info['parallel_tasks'],
            failed_tasks=parallel_info['failed_tasks'],
            optimization_suggestions=optimization_suggestions
        )
        
        if output_callback:
            output_callback(f"âœ… ä¼˜åŒ–æ£€æµ‹å®Œæˆï¼Œè€—æ—¶ {metrics.total_time:.2f}s")
        
        return result
    
    def _execute_parallel_detection(self, 
                                   script_path: str,
                                   output_callback: Optional[Callable[[str], None]],
                                   use_execution: bool) -> Tuple[IntelligentAnalysisResult, Dict]:
        """å¹¶è¡Œæ‰§è¡Œæ£€æµ‹"""
        
        if output_callback:
            output_callback(f"ğŸ”„ å¯åŠ¨ {self.max_workers} ä¸ªå¹¶è¡Œä»»åŠ¡...")
        
        # ä½¿ç”¨æ™ºèƒ½åˆ†æå™¨
        analysis_result = self.intelligent_analyzer.analyze_script(
            script_path=script_path,
            output_callback=output_callback,
            use_execution=use_execution,
            enable_ml_scoring=True
        )
        
        parallel_info = {
            'parallel_tasks': self.max_workers,
            'failed_tasks': 0,
            'successful_tasks': self.max_workers
        }
        
        return analysis_result, parallel_info
    
    def _generate_cache_key(self, script_path: str, use_execution: bool) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        try:
            # åŸºäºæ–‡ä»¶å†…å®¹ã€ä¿®æ”¹æ—¶é—´å’Œé…ç½®ç”Ÿæˆé”®
            with open(script_path, 'rb') as f:
                content_hash = hashlib.sha256(f.read()).hexdigest()
            
            mtime = os.path.getmtime(script_path)
            config_hash = hashlib.md5(
                f"{self.python_interpreter}_{use_execution}_{self.max_workers}".encode()
            ).hexdigest()
            
            return f"optimized_{content_hash}_{mtime}_{config_hash}"
        
        except Exception:
            # å›é€€åˆ°ç®€å•çš„é”®ç”Ÿæˆ
            return f"optimized_{script_path}_{use_execution}_{int(time.time())}"
    
    def _async_cache_result(self, cache_key: str, result: IntelligentAnalysisResult):
        """å¼‚æ­¥ç¼“å­˜ç»“æœ"""
        def cache_worker():
            try:
                # è®¾ç½®è¾ƒé•¿çš„TTLï¼Œå› ä¸ºåˆ†æç»“æœç›¸å¯¹ç¨³å®š
                self.cache_manager.set(
                    key=cache_key,
                    data=result,
                    ttl=7200,  # 2å°æ—¶
                    tags=['module_detection', 'intelligent_analysis']
                )
            except Exception as e:
                # ç¼“å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                pass
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¼‚æ­¥æ‰§è¡Œ
        ThreadPoolExecutor(max_workers=1).submit(cache_worker)
    
    def _calculate_parallel_efficiency(self, detection_time: float, parallel_tasks: int) -> float:
        """è®¡ç®—å¹¶è¡Œæ•ˆç‡"""
        if parallel_tasks <= 1:
            return 100.0
        
        # ç®€åŒ–çš„å¹¶è¡Œæ•ˆç‡è®¡ç®—
        # å®é™…åº”è¯¥åŸºäºç†è®ºæœ€ä¼˜æ—¶é—´å’Œå®é™…æ—¶é—´çš„æ¯”è¾ƒ
        theoretical_time = detection_time * parallel_tasks
        actual_time = detection_time
        
        if theoretical_time > 0:
            efficiency = (theoretical_time - actual_time) / theoretical_time * 100
            return max(0.0, min(100.0, efficiency))
        
        return 0.0
    
    def _generate_optimization_suggestions(self, 
                                         metrics: PerformanceMetrics,
                                         analysis_result: IntelligentAnalysisResult,
                                         parallel_info: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # æ€§èƒ½å»ºè®®
        if metrics.total_time > 10.0:
            suggestions.append("æ£€æµ‹è€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®å¯ç”¨æ›´å¤šå¹¶è¡Œä»»åŠ¡æˆ–ä¼˜åŒ–è„šæœ¬")
        
        if metrics.cache_hit_rate < 0.3:
            suggestions.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ ç¼“å­˜TTLæˆ–æ£€æŸ¥æ–‡ä»¶å˜æ›´é¢‘ç‡")
        
        if metrics.memory_usage > 500 * 1024 * 1024:  # 500MB
            suggestions.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®å‡å°‘å¹¶è¡Œä»»åŠ¡æ•°æˆ–ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")
        
        if parallel_info['failed_tasks'] > 0:
            suggestions.append(f"æœ‰ {parallel_info['failed_tasks']} ä¸ªå¹¶è¡Œä»»åŠ¡å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿèµ„æº")
        
        # æ£€æµ‹ç»“æœå»ºè®®
        if len(analysis_result.recommended_modules) > 100:
            suggestions.append("æ£€æµ‹åˆ°å¤§é‡æ¨¡å—ï¼Œå»ºè®®æ£€æŸ¥ä¾èµ–å…³ç³»æˆ–ä½¿ç”¨æ›´ç²¾ç¡®çš„æ£€æµ‹æ–¹æ³•")
        
        if len(analysis_result.risky_modules) > 10:
            suggestions.append("æ£€æµ‹åˆ°è¾ƒå¤šé£é™©æ¨¡å—ï¼Œå»ºè®®ä»”ç»†å®¡æŸ¥è¿™äº›ä¾èµ–")
        
        return suggestions
    
    def _warmup_cache(self):
        """é¢„çƒ­ç¼“å­˜"""
        # é¢„åŠ è½½ä¸€äº›å¸¸ç”¨çš„æ¨¡å—ä¿¡æ¯åˆ°ç¼“å­˜
        common_modules = [
            'os', 'sys', 'json', 'time', 'datetime', 'pathlib',
            'collections', 'itertools', 'functools', 'threading'
        ]
        
        for module in common_modules:
            cache_key = f"module_info_{module}"
            if not self.cache_manager.get(cache_key):
                # ç¼“å­˜åŸºæœ¬æ¨¡å—ä¿¡æ¯
                module_info = {
                    'name': module,
                    'is_standard': True,
                    'category': 'standard_library'
                }
                self.cache_manager.set(cache_key, module_info, ttl=86400)  # 24å°æ—¶
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        with self.performance_lock:
            avg_time = self.total_time / self.total_requests if self.total_requests > 0 else 0.0
            cache_hit_rate = self.cache_hits / self.total_requests if self.total_requests > 0 else 0.0
        
        cache_stats = self.cache_manager.get_stats()
        
        return {
            'total_requests': self.total_requests,
            'average_time': avg_time,
            'cache_hit_rate': cache_hit_rate,
            'cache_stats': cache_stats,
            'max_workers': self.max_workers,
            'enable_process_pool': self.enable_process_pool
        }
    
    def optimize_cache(self):
        """ä¼˜åŒ–ç¼“å­˜"""
        # æ¸…ç†è¿‡æœŸæ¡ç›®
        self.cache_manager._cleanup_expired()
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.cache_manager.get_stats()
        
        # å¦‚æœå‘½ä¸­ç‡å¤ªä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç­–ç•¥
        if stats.hit_rate < 0.3:
            # å¢åŠ ç¼“å­˜TTL
            pass
        
        # å¦‚æœå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œæ¸…ç†ä¸€äº›æ¡ç›®
        if stats.total_memory_size > 150 * 1024 * 1024:  # 150MB
            # å¯ä»¥å®ç°æ›´æ™ºèƒ½çš„æ¸…ç†ç­–ç•¥
            pass
    
    def clear_cache(self, tags: Optional[List[str]] = None) -> int:
        """æ¸…ç†ç¼“å­˜"""
        return self.cache_manager.clear(tags)
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        # ç¡®ä¿ç¼“å­˜ç®¡ç†å™¨æ­£ç¡®å…³é—­
        if hasattr(self, 'cache_manager'):
            del self.cache_manager
